{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Census"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
||||||| merged common ancestors
   "execution_count": 1,
=======
   "execution_count": 10,
>>>>>>> Rephrasing
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "path_to_csv = \"adult.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
||||||| merged common ancestors
   "execution_count": 3,
=======
   "execution_count": 11,
>>>>>>> Rephrasing
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"There are 11 features and 1 target variable - the salary. A good observation is that the target data is not evenly split: 12360 out of 16281 samples are negative. Given that, using solely accuracy, recall, precision or specificity will not give us a good metric for our model. The problem with it is that it equalizes the missclasification. Usually the costs are not the same and having in mind it is not specified, it is better to go for AUC/ROC Curve. AUC/ROC Curve is really useful instrument which can give us good visualization of both cases of missclasification. As long as we do not need exact probability of an entry belonging to target variable 0 or 1, we are good to go.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Using gender and race can lead to really biased algorithm. There are already existing cases where people's CVs were discarded based only on the gender. These 2 features should not affect our algorithm as they should not play a role in the salary a person receives. We can use them for statistics, but not for decision-making. Given that we believe that gender and race should not be included.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        education  education-num\n0            10th            6.0\n1            11th            7.0\n2            12th            8.0\n3         1st-4th            2.0\n4         5th-6th            3.0\n..            ...            ...\n11        HS-grad            9.0\n12        Masters           14.0\n13      Preschool            1.0\n14    Prof-school           15.0\n15   Some-college           10.0\n\n[16 rows x 2 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'native-country: most likely user did not want to give input, we leave it like that'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df = pd.read_csv(\"X_train.csv\")\n",
    "df = df.dropna(subset=[\"occupation\"])\n",
    "no_hours_df = df.dropna(subset=[\"education-num\"])\n",
    "means = no_hours_df.groupby('education')['education-num'].mean()\n",
    "\n",
    "print(means.reset_index())\n",
    "means_dict = means.to_dict()\n",
    "\n",
    "df['education-num'] = df.apply(\n",
    "    lambda row: means_dict[row['education']] if np.isnan(row['education-num']) else row['education-num'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "\"There are missing values in 4 columns: education-num(240), workclass(936), occupation(1181) and native-country(300).\"\n",
    "\"'education-num': most likely user forgot to give input, we estimate (take the average) based on type of education. So if a person has type of education 'Masters', we set education-num as the average of all the people who did 'Masters'.\"\n",
    "\"'workclass': empty because there is no occupation, we drop all rows.\"\n",
    "\"'occupation': most likely the person is unemployed, we drop all rows, as people who have 0 income are irrelevant.\"\n",
    "\"'native-country': most likely user did not want to give input, we leave it like that.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{' United-States': 0, ' Germany': 1, ' Canada': 2, nan: 3, ' Vietnam': 4, ' Poland': 5, ' Mexico': 6, ' Hungary': 7, ' Ecuador': 8, ' Philippines': 9, ' Japan': 10, ' England': 11, ' El-Salvador': 12, ' Guatemala': 13, ' China': 14, ' India': 15, ' Cuba': 16, ' Honduras': 17, ' South': 18, ' Puerto-Rico': 19, ' Columbia': 20, ' France': 21, ' Greece': 22, ' Dominican-Republic': 23, ' Nicaragua': 24, ' Italy': 25, ' Cambodia': 26, ' Haiti': 27, ' Trinadad&Tobago': 28, ' Jamaica': 29, ' Iran': 30, ' Laos': 31, ' Portugal': 32, ' Taiwan': 33, ' Yugoslavia': 34, ' Peru': 35, ' Outlying-US(Guam-USVI-etc)': 36, ' Ireland': 37, ' Thailand': 38, ' Hong': 39, ' Scotland': 40}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The features that were transformed are 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'. All the unique values for each parameter are extracted and a dictionary is created with structure like {0: 'unique_value_1'}. Then the values are replaced with the indeces for a scalable solution.\""
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "workclass = df['workclass'].unique()\n",
    "workclass = {k: v for v, k in enumerate(workclass)}\n",
    "\n",
    "education = df['education'].unique()\n",
    "education = {k: v for v, k in enumerate(education)}\n",
    "\n",
    "marital = df['marital-status'].unique()\n",
    "marital = {k: v for v, k in enumerate(marital)}\n",
    "\n",
    "occupation = df['occupation'].unique()\n",
    "occupation = {k: v for v, k in enumerate(occupation)}\n",
    "\n",
    "relationship = df['relationship'].unique()\n",
    "relationship = {k: v for v, k in enumerate(relationship)}\n",
    "\n",
    "race = df['race'].unique()\n",
    "race = {k: v for v, k in enumerate(race)}\n",
    "\n",
    "sex = df['sex'].unique()\n",
    "sex = {k: v for v, k in enumerate(sex)}\n",
    "\n",
    "country = df['native-country'].unique()\n",
    "country = {k: v for v, k in enumerate(country)}\n",
    "\n",
    "print(country)\n",
    "df['workclass'].replace(workclass, inplace=True)\n",
    "df['education'].replace(education, inplace=True)\n",
    "df['marital-status'].replace(marital, inplace=True)\n",
    "df['occupation'].replace(occupation, inplace=True)\n",
    "df['relationship'].replace(relationship, inplace=True)\n",
    "df['race'].replace(race, inplace=True)\n",
    "df['sex'].replace(sex, inplace=True)\n",
    "df['native-country'].replace(country, inplace=True)\n",
    "\n",
    "\n",
    "\"The features that were transformed are 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country'. All the unique values for each parameter are extracted and a dictionary is created with structure like {'unique_value_1': 0}. Then the values are replaced with the corresponding index. This way the solution is scalable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "prediction = np.array([-1] * len(X_test)) #TODO replace this with you own prediction\n",
    "pd.DataFrame(prediction).to_csv(\"GROUP_classes_problem_census.txt\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}